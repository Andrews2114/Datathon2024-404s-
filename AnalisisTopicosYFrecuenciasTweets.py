# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDSN3EGKoUMYPdB97SAqsfpQj-UvY495
"""

# Importar bibliotecas
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import gensim.corpora as corpora
from gensim.models import LdaModel
from google.colab import files  # Importar el módulo files de google.colab
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Definir función de preprocesamiento de texto
def preprocess_text(text):
    stop_words = set(stopwords.words('spanish'))
    tokens = word_tokenize(text.lower())
    tokens = [token for token in tokens if token.isalpha()]
    tokens = [token for token in tokens if token not in stop_words]
    return tokens

# Solicitar al usuario cargar archivo CSV
uploaded = files.upload()

# Abrir y mostrar los primeros registros del archivo CSV
print("--------------------------------Registros del archivo")
for fn in uploaded.keys():
    df = pd.read_csv(fn, header=0, names=['date', 'time', 'tweet'])
    print(df.head())

    # Preprocesamiento de texto
    df['processed_tweet'] = df['tweet'].apply(preprocess_text)

    # Crear diccionario y corpus para modelo de LDA
    dictionary = corpora.Dictionary(df['processed_tweet'])
    corpus = [dictionary.doc2bow(text) for text in df['processed_tweet']]

    # Entrenar modelo de LDA
    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5)

    # Mostrar temas identificados por LDA

    print("--------------------------------Temas identificados por el modelo LDA realizado")
    for idx, topic in lda_model.print_topics():
        print('Tema {}: {}'.format(idx, topic))

    import matplotlib.pyplot as plt

# Datos de ejemplo
topic_data = lda_model.print_topics()

# Función para extraer las palabras clave y sus pesos
def extract_keywords(topic_data):
    keywords = []
    weights = []

    for topic, keywords_str in topic_data:
        keyword_weight_pairs = [pair.split("*") for pair in keywords_str.split(" + ")]
        keywords.append([pair[1].strip('"') for pair in keyword_weight_pairs])
        weights.append([float(pair[0]) for pair in keyword_weight_pairs])

    return keywords, weights

# Extraer palabras clave y pesos
keywords, weights = extract_keywords(topic_data)

# Calcular porcentajes de pesos para cada tópico
total_weights = [sum(topic_weights) for topic_weights in weights]
topic_percentages = [[weight / total_weight * 100 for weight in topic_weights] for total_weight, topic_weights in zip(total_weights, weights)]

# Colores para las porciones del pastel
colors = plt.cm.tab20.colors

import networkx as nx

# Crear un grafo vacío
G = nx.Graph()

# Agregar nodos al grafo (palabras clave de los tópicos)
for i, topic_keywords in enumerate(keywords):
    for keyword in topic_keywords:
        G.add_node(keyword, topic=f"Tópico {i}")

# Crear conexiones entre nodos de palabras clave de diferentes tópicos
for i in range(len(keywords)):
    for j in range(i+1, len(keywords)):
        for keyword_i in keywords[i]:
            for keyword_j in keywords[j]:
                G.add_edge(keyword_i, keyword_j)

# Dibujar el gráfico de red sin etiquetas de bordes
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G, seed=42)  # Definir una disposición de los nodos

# Calcular la frecuencia de cada palabra en el corpus
word_frequencies = {}
for text in df['processed_tweet']:
    for word in text:
        if word in word_frequencies:
            word_frequencies[word] += 1
        else:
            word_frequencies[word] = 1

# Obtener la frecuencia máxima y mínima
max_freq = max(word_frequencies.values())
min_freq = min(word_frequencies.values())

# Escalar las frecuencias a un rango deseado (por ejemplo, entre 100 y 3000 para el tamaño de los nodos)
scaled_node_sizes = {}
for word, freq in word_frequencies.items():
    scaled_size = 1000 + (freq - min_freq) * (10000 - 100) / (max_freq - min_freq)
    scaled_node_sizes[word] = scaled_size


# Dibujar el gráfico de red con tamaños de nodos escalados según la frecuencia de las palabras
print("--------------------------------Gráfico sobre palabras más frecuentes")
plt.figure(figsize=(12, 8))
nx.draw(G, pos, with_labels=True, node_size=[scaled_node_sizes[word] for word in G.nodes()], node_color='lightblue', font_size=10, font_weight='bold')
plt.title('Network Graph de Tópicos LDA')
plt.show()

"""Análisis de frecuencias palabras

"""

# Importar bibliotecas
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
import matplotlib.pyplot as plt

# Cargar archivo CSV

# Tokenización y eliminación de palabras vacías y signos de puntuación
stop_words = set(stopwords.words('spanish'))  # Asumiendo que el idioma es español
punctuation = set(string.punctuation)
custom_stop_words = set(['rt', 'http', 'https', 'si'])  # Palabras personalizadas para eliminar


def preprocess_text(text):
    tokens = word_tokenize(text.lower()) # Tokenización y conversión a minúsculas
    tokens = [word for word in tokens if word.isalpha()] # Eliminar números y otros caracteres no alfabéticos
    tokens = [word for word in tokens if word not in stop_words] # Eliminar palabras vacías
    tokens = [word for word in tokens if word not in punctuation] # Eliminar signos de puntuación
    tokens = [word for word in tokens if word not in custom_stop_words] # Eliminar palabras personalizadas
    return tokens

# Preprocesamiento de texto para cada tweet
df['processed_tweet'] = df['tweet'].apply(preprocess_text)

# Conteo de frecuencia de palabras
all_words = [word for tweet_words in df['processed_tweet'] for word in tweet_words] # Lista de todas las palabras
word_freq = pd.Series(all_words).value_counts().reset_index() # Conteo de frecuencia de palabras
word_freq.columns = ['word', 'frequency'] # Renombrar columnas

# Mostrar las 60 palabras más frecuentes
print("Palabras más comunes y con sentido según NLTK")
print(word_freq.head(60))

#Se han seleccionado las palabras más relacionadas al contexto de hey banco y que podrían tener un significado relevante para interpretar los comentarios de los usuarios.
palabras=['gracias','contratación','muchas','heybanco','hey','credito','problema','ayuda','tarjeta','hola','rápido','dinero','banco','servicio','app','comisiones','información']
# Crear un diccionario para almacenar la frecuencia de cada palabra en 'palabras'
palabras_freq = {}

# Iterar sobre cada palabra en 'palabras' y obtener su frecuencia
print("\nPalabras más frecuentes con sentido")
for palabra in palabras:
    if palabra in word_freq['word'].values:
        palabras_freq[palabra] = word_freq[word_freq['word'] == palabra]['frequency'].values[0]
    else:
        palabras_freq[palabra] = 0

# Mostrar el conteo de frecuencia de cada palabra en 'palabras'
for palabra, frecuencia in palabras_freq.items():
    print(f"{palabra}: {frecuencia}")

# Graficar un gráfico de barras con la frecuencia de las palabras en 'palabras'
plt.figure(figsize=(10, 6))
plt.bar(palabras_freq.keys(), palabras_freq.values(), color='skyblue')
plt.xlabel('Palabra')
plt.ylabel('Frecuencia')
plt.title('Frecuencia de Palabras')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()